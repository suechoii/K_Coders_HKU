{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### csv 형식으로 된 데이터를 학습 데이터와 테스트 데이터로 나눈 후, tensorflow로 Softmax Classification을 구현하여 학습 및 성능 평가까지 해보고자 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (데이터는 \"https://github.com/hunkim/DeepLearningZeroToAll/blob/master/data-04-zoo.csv\"에서 받을 수 있다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##tenssorflow==2.3 환경에서 구현  \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "all_data = np.loadtxt('./data-04-zoo.csv', delimiter=',', dtype=np.float32)\n",
    "\n",
    "class_number = 7  \n",
    "feature_number=16\n",
    "\n",
    "train_x = all_data[:-10,0:-1]\n",
    "train_y = all_data[:-10,[-1]]\n",
    "\n",
    "test_x = all_data[-10:,0:-1]\n",
    "test_y = all_data[-10:,[-1]]\n",
    "#데이터의 one hot encoding\n",
    "train_y_one_hot = tf.one_hot(np.array(train_y), class_number)  \n",
    "final_train_y= tf.reshape(train_y_one_hot,[-1,class_number])\n",
    "\n",
    "test_y_one_hot = tf.one_hot(np.array(test_y), class_number)  \n",
    "final_test_y = tf.reshape(test_y_one_hot,[-1,class_number])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class 수와 피쳐 수에 따라서 weigh와 bias 변수 설정\n",
    "w = tf.Variable(tf.random.uniform([feature_number,class_number]))\n",
    "b = tf.Variable(tf.random.uniform([class_number]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss_value: 2.7715318\n",
      "100 loss_value: 0.2914459\n",
      "200 loss_value: 0.14741716\n",
      "300 loss_value: 0.09252634\n",
      "400 loss_value: 0.06389113\n",
      "500 loss_value: 0.046870615\n",
      "600 loss_value: 0.035912897\n",
      "700 loss_value: 0.028440863\n",
      "800 loss_value: 0.023112278\n",
      "900 loss_value: 0.01917276\n",
      "1000 loss_value: 0.016172737\n",
      "1100 loss_value: 0.013831398\n",
      "1200 loss_value: 0.011965955\n",
      "1300 loss_value: 0.010453417\n",
      "1400 loss_value: 0.009208323\n",
      "1500 loss_value: 0.008170009\n",
      "1600 loss_value: 0.007294216\n",
      "1700 loss_value: 0.006548092\n",
      "1800 loss_value: 0.005906751\n",
      "1900 loss_value: 0.005351185\n",
      "2000 loss_value: 0.0048664208\n",
      "2100 loss_value: 0.0044408524\n",
      "2200 loss_value: 0.004065048\n",
      "2300 loss_value: 0.0037314834\n",
      "2400 loss_value: 0.003434003\n",
      "2500 loss_value: 0.003167556\n",
      "2600 loss_value: 0.0029279853\n",
      "2700 loss_value: 0.0027117631\n",
      "2800 loss_value: 0.0025159458\n",
      "2900 loss_value: 0.0023381216\n",
      "3000 loss_value: 0.0021761262\n",
      "3100 loss_value: 0.0020281933\n",
      "3200 loss_value: 0.0018927291\n",
      "3300 loss_value: 0.0017684352\n",
      "3400 loss_value: 0.0016541466\n",
      "3500 loss_value: 0.0015488295\n",
      "3600 loss_value: 0.00145163\n",
      "3700 loss_value: 0.0013617226\n",
      "3800 loss_value: 0.001278469\n",
      "3900 loss_value: 0.0012012543\n",
      "4000 loss_value: 0.0011294882\n",
      "4100 loss_value: 0.0010627653\n",
      "4200 loss_value: 0.0010006315\n",
      "4300 loss_value: 0.0009426776\n",
      "4400 loss_value: 0.000888578\n",
      "4500 loss_value: 0.0008380331\n",
      "4600 loss_value: 0.00079075876\n",
      "4700 loss_value: 0.0007465011\n",
      "4800 loss_value: 0.00070502365\n",
      "4900 loss_value: 0.0006661352\n",
      "5000 loss_value: 0.000629625\n",
      "5100 loss_value: 0.00059534825\n",
      "5200 loss_value: 0.0005631235\n",
      "5300 loss_value: 0.0005327952\n",
      "5400 loss_value: 0.000504294\n",
      "5500 loss_value: 0.00047741953\n",
      "5600 loss_value: 0.00045212687\n",
      "5700 loss_value: 0.00042826118\n",
      "5800 loss_value: 0.00040575903\n",
      "5900 loss_value: 0.00038454664\n",
      "6000 loss_value: 0.00036450272\n",
      "6100 loss_value: 0.00034559515\n",
      "6200 loss_value: 0.00032773527\n",
      "6300 loss_value: 0.00031083982\n",
      "6400 loss_value: 0.00029487503\n",
      "6500 loss_value: 0.00027974966\n",
      "6600 loss_value: 0.00026546651\n",
      "6700 loss_value: 0.0002519448\n",
      "6800 loss_value: 0.00023915058\n",
      "6900 loss_value: 0.00022700947\n",
      "7000 loss_value: 0.00021552296\n",
      "7100 loss_value: 0.00020465718\n",
      "7200 loss_value: 0.00019437296\n",
      "7300 loss_value: 0.00018459844\n",
      "7400 loss_value: 0.00017533763\n",
      "7500 loss_value: 0.00016657099\n",
      "7600 loss_value: 0.00015825279\n",
      "7700 loss_value: 0.00015034253\n",
      "7800 loss_value: 0.00014283502\n",
      "7900 loss_value: 0.00013574082\n",
      "8000 loss_value: 0.00012900888\n",
      "8100 loss_value: 0.00012259478\n",
      "8200 loss_value: 0.00011652472\n",
      "8300 loss_value: 0.00011074506\n",
      "8400 loss_value: 0.00010528202\n",
      "8500 loss_value: 0.00010007933\n",
      "8600 loss_value: 9.5152725e-05\n",
      "8700 loss_value: 9.0457725e-05\n",
      "8800 loss_value: 8.5998254e-05\n",
      "8900 loss_value: 8.177699e-05\n",
      "9000 loss_value: 7.773236e-05\n",
      "9100 loss_value: 7.3923344e-05\n",
      "9200 loss_value: 7.028838e-05\n",
      "9300 loss_value: 6.686025e-05\n",
      "9400 loss_value: 6.357346e-05\n",
      "9500 loss_value: 6.047519e-05\n",
      "9600 loss_value: 5.7497316e-05\n",
      "9700 loss_value: 5.4694898e-05\n",
      "9800 loss_value: 5.200241e-05\n",
      "9900 loss_value: 4.946967e-05\n"
     ]
    }
   ],
   "source": [
    "#loss function 정의\n",
    "def loss_function():\n",
    "#softmax_classification_1에서와 달리, tf.nn.softmax_cross_entropy_with_logits을 활용하여 categorical cross entropy를 간편하게 구현\n",
    "  z= tf.matmul(train_x,w)+b\n",
    "  softmax_result = tf.nn.softmax_cross_entropy_with_logits(logits=z,labels=final_train_y)\n",
    "  cost = tf.reduce_mean(softmax_result)  \n",
    "  return cost\n",
    "optimizer = tf.optimizers.Adam(learning_rate=0.01) #보편적으로 가장 많이 사용하는 adam optimizer 활용\n",
    "for step in range(10000): #train\n",
    "    cost_val=optimizer.minimize(loss_function, var_list=[w,b])\n",
    "    if step % 100 == 0:\n",
    "        print(step,\"loss_value:\", loss_function().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "#최종 train accuracy 측정\n",
    "pred_y = tf.nn.softmax(tf.matmul(train_x,w)+b)\n",
    "argmax_y=tf.argmax(final_train_y,1) #label 데이터의 argmax\n",
    "pred_final = tf.argmax(pred_y,1)#모델이 예측한 pred_y 데이터의 argmax\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred_final,argmax_y),dtype=tf.float32)) #train_accuracy\n",
    "print(\"Train accuracy:\", accuracy.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "#test data 설정 및 학습한 모델로 predict\n",
    "test_pred= tf.nn.softmax(tf.matmul(test_x,w)+b)\n",
    "test_argmax_y=tf.argmax(final_test_y ,1)\n",
    "test_pred_final= tf.argmax(test_pred,1) \n",
    "test_accuracy = tf.reduce_mean(tf.cast(tf.equal(test_pred_final,test_argmax_y),dtype=tf.float32)) #test_accuracy\n",
    "print(\"Test accuracy:\", test_accuracy.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
